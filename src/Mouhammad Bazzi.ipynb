{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Mouhammad BAZZI\n",
    "\n",
    "Student ID : A20522180\n",
    "\n",
    "CS577 - Fall 2022</br> <h1><br><b><font color='red'>Assignement 2</font></br></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b><font color='ORANGE'>QUESTION 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions for question 1:\n",
    "#Implement and test a three layer neural network for a three class classification problem.\n",
    "#Use categorical cross entropy as the loss function.\n",
    "#Use the sigmoid activation function for the hidden layer and the softmax activation function for the output layer.\n",
    "#Evaluate the network implented.\n",
    "#The code should be modular and well documented.\n",
    "#Use explicit training loop while performing the following tasks:\n",
    "#1. Iterates over batches of data\n",
    "#2. Computes the forward pass\n",
    "#3. Computes the loss (backpropagation with tf.GradientTape)\n",
    "#4. Apply the computed gradients to the network parameters\n",
    "#5. Compute and print the training loss and accuracy for each epoch\n",
    "#6. Compute and print the test loss and accuracy for each epoch\n",
    "\n",
    "#############################################\n",
    "#Importing the libraries\n",
    "#############################################\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The next two cells are specific to the CIFAR10 DataSet </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# CIFAR10 - Implementation of the neural network \n",
    "#############################################\n",
    "\n",
    "#Defining the model\n",
    "input = keras.Input(shape=(32*32*3,))\n",
    "hidden1 = layers.Dense(100, activation='sigmoid')(input)\n",
    "hidden2 = layers.Dense(100, activation='sigmoid')(hidden1)\n",
    "output = layers.Dense(3, activation='softmax')(hidden2)\n",
    "model = keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "#Instantiating the optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "#Instantiating the loss function\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# CIFAR10 - Preprocessing of the data\n",
    "############################################\n",
    "\n",
    "#Loading the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#Selection of a subset of 3 classes of the dataset\n",
    "def select_subset(x_train, y_train, classes):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    for i in range((len(x_train))):\n",
    "        for j in range(len(classes)):\n",
    "            if y_train[i] == classes[j]:\n",
    "                x_train_temp.append(x_train[i])\n",
    "                y_train_temp.append(y_train[i])\n",
    "    return np.array(x_train_temp), np.array(y_train_temp)\n",
    "\n",
    "#Selection of the classes\n",
    "classes = [0, 1, 2]\n",
    "x_train, y_train = select_subset(x_train, y_train, classes)\n",
    "x_test, y_test = select_subset(x_test, y_test, classes)\n",
    "\n",
    "#Order the training data by class by returning an array of n classes\n",
    "def order_data(x_train, y_train):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    for i in range(len(classes)):\n",
    "        x_train_temp.append([])\n",
    "        y_train_temp.append([])\n",
    "    for i in range(len(x_train)):\n",
    "        for j in range(len(classes)):\n",
    "            if y_train[i] == classes[j]:\n",
    "                x_train_temp[j].append(x_train[i])\n",
    "                y_train_temp[j].append(y_train[i])\n",
    "    return x_train_temp, y_train_temp\n",
    "\n",
    "x_train_ordered, y_train_ordered = order_data(x_train, y_train)\n",
    "\n",
    "#Splitting the dataset into the Training set and Validation set\n",
    "#Training set and Validation set should have the same distribution of classes\n",
    "#Split ratio: 80% training set, 20% validation set\n",
    "def split_data(x_train_ordered, y_train_ordered, split_ratio):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    x_val_temp = []\n",
    "    y_val_temp = []\n",
    "    for i in range(len(x_train_ordered)):\n",
    "        for j in range(len(x_train_ordered[i])):\n",
    "            if j < len(x_train_ordered[i])*split_ratio:\n",
    "                x_train_temp.append(x_train_ordered[i][j])\n",
    "                y_train_temp.append(y_train_ordered[i][j])\n",
    "            else:\n",
    "                x_val_temp.append(x_train_ordered[i][j])\n",
    "                y_val_temp.append(y_train_ordered[i][j])\n",
    "    return np.array(x_train_temp), np.array(y_train_temp), np.array(x_val_temp), np.array(y_val_temp)\n",
    "\n",
    "\n",
    "x_train, y_train, x_val, y_val = split_data(x_train_ordered, y_train_ordered, 0.8)\n",
    "\n",
    "#Shuffling the training set and the validation set\n",
    "def shuffle_data(x_train, y_train, x_val, y_val):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    x_val_temp = []\n",
    "    y_val_temp = []\n",
    "    for i in range(len(x_train)):\n",
    "        x_train_temp.append(x_train[i])\n",
    "        y_train_temp.append(y_train[i])\n",
    "    for i in range(len(x_val)):\n",
    "        x_val_temp.append(x_val[i])\n",
    "        y_val_temp.append(y_val[i])\n",
    "    np.random.shuffle(x_train_temp)\n",
    "    np.random.shuffle(y_train_temp)\n",
    "    np.random.shuffle(x_val_temp)\n",
    "    np.random.shuffle(y_val_temp)\n",
    "    return np.array(x_train_temp), np.array(y_train_temp), np.array(x_val_temp), np.array(y_val_temp)\n",
    "\n",
    "x_train, y_train, x_val, y_val = shuffle_data(x_train, y_train, x_val, y_val)\n",
    "\n",
    "\n",
    "#Reshaping the images\n",
    "x_train = x_train.reshape(x_train.shape[0], 32*32*3)\n",
    "x_val = x_val.reshape(x_val.shape[0], 32*32*3)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32*32*3)\n",
    "\n",
    "#Encoding the labels using categorical encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=3)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "\n",
    "#Batching the dataset\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The next two cells are specific to the MNIST DataSet </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# MNIST - Implementation of the neural network \n",
    "#############################################\n",
    "\n",
    "#Defining the model\n",
    "input = keras.Input(shape=(28*28,))\n",
    "hidden1 = layers.Dense(100, activation='sigmoid')(input)\n",
    "hidden2 = layers.Dense(100, activation='sigmoid')(hidden1)\n",
    "output = layers.Dense(3, activation='softmax')(hidden2)\n",
    "model = keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "#Instantiating the optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "#Instantiating the loss function\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# MNIST - Preprocessing of the data\n",
    "############################################\n",
    "\n",
    "#Loading the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "#Selection of a subset of 3 classes of the dataset\n",
    "def select_subset(x_train, y_train, classes):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    for i in range((len(x_train))):\n",
    "        for j in range(len(classes)):\n",
    "            if y_train[i] == classes[j]:\n",
    "                x_train_temp.append(x_train[i])\n",
    "                y_train_temp.append(y_train[i])\n",
    "    return np.array(x_train_temp), np.array(y_train_temp)\n",
    "\n",
    "#Selection of the classes\n",
    "classes = [0, 1, 2]\n",
    "x_train, y_train = select_subset(x_train, y_train, classes)\n",
    "x_test, y_test = select_subset(x_test, y_test, classes)\n",
    "\n",
    "#Order the training data by class by returning an array of n classes\n",
    "def order_data(x_train, y_train):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    for i in range(len(classes)):\n",
    "        x_train_temp.append([])\n",
    "        y_train_temp.append([])\n",
    "    for i in range(len(x_train)):\n",
    "        for j in range(len(classes)):\n",
    "            if y_train[i] == classes[j]:\n",
    "                x_train_temp[j].append(x_train[i])\n",
    "                y_train_temp[j].append(y_train[i])\n",
    "    return x_train_temp, y_train_temp\n",
    "\n",
    "x_train_ordered, y_train_ordered = order_data(x_train, y_train)\n",
    "\n",
    "#Splitting the dataset into the Training set and Validation set\n",
    "#Training set and Validation set should have the same distribution of classes\n",
    "#Split ratio: 80% training set, 20% validation set\n",
    "def split_data(x_train_ordered, y_train_ordered, split_ratio):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    x_val_temp = []\n",
    "    y_val_temp = []\n",
    "    for i in range(len(x_train_ordered)):\n",
    "        for j in range(len(x_train_ordered[i])):\n",
    "            if j < len(x_train_ordered[i])*split_ratio:\n",
    "                x_train_temp.append(x_train_ordered[i][j])\n",
    "                y_train_temp.append(y_train_ordered[i][j])\n",
    "            else:\n",
    "                x_val_temp.append(x_train_ordered[i][j])\n",
    "                y_val_temp.append(y_train_ordered[i][j])\n",
    "    return np.array(x_train_temp), np.array(y_train_temp), np.array(x_val_temp), np.array(y_val_temp)\n",
    "\n",
    "\n",
    "x_train, y_train, x_val, y_val = split_data(x_train_ordered, y_train_ordered, 0.8)\n",
    "\n",
    "#Shuffling the training set and the validation set\n",
    "def shuffle_data(x_train, y_train, x_val, y_val):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    x_val_temp = []\n",
    "    y_val_temp = []\n",
    "    for i in range(len(x_train)):\n",
    "        x_train_temp.append(x_train[i])\n",
    "        y_train_temp.append(y_train[i])\n",
    "    for i in range(len(x_val)):\n",
    "        x_val_temp.append(x_val[i])\n",
    "        y_val_temp.append(y_val[i])\n",
    "    np.random.shuffle(x_train_temp)\n",
    "    np.random.shuffle(y_train_temp)\n",
    "    np.random.shuffle(x_val_temp)\n",
    "    np.random.shuffle(y_val_temp)\n",
    "    return np.array(x_train_temp), np.array(y_train_temp), np.array(x_val_temp), np.array(y_val_temp)\n",
    "\n",
    "x_train, y_train, x_val, y_val = shuffle_data(x_train, y_train, x_val, y_val)\n",
    "\n",
    "#Reshaping the images\n",
    "x_train = x_train.reshape(x_train.shape[0], 28*28)\n",
    "x_val = x_val.reshape(x_val.shape[0], 28*28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28*28)\n",
    "\n",
    "#Encoding the labels using categorical encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=3)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "#Batching the dataset\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>EXECUTE THE NEXT CELL ONLY IF YOU WANT TO TRAIN ON THE ENTIRE TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#For Training the model using the entire dataset\n",
    "############################################\n",
    "\n",
    "#Training the model using the entire dataset by merging the training and validation sets\n",
    "x_train = np.concatenate((x_train, x_val), axis=0)\n",
    "y_train = np.concatenate((y_train, y_val), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Start of the training process </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Training the model and plotting the results\n",
    "############################################\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "#Open a GradientTape to record the operations run during the forward pass, which enables autodifferentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        #Computes the forward pass\n",
    "        #The operations that the layer applies to its inputs are going to be recorded on the GradientTape\n",
    "        logits = model(x, training=True) # Logits for this minibatch\n",
    "        #Computes the loss value for this minibatch\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    #Use the gradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    #Apply the gradients to the model's parameters\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    #Update the metrics tracking the loss and accuracy\n",
    "    train_accuracy_results.update_state(y, logits)\n",
    "    train_loss_results.update_state(loss_value)\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    #Computes the forward pass\n",
    "    val_logits = model(x, training=False)\n",
    "    #Computes the loss value for this minibatch\n",
    "    val_loss_value = loss_fn(y, val_logits)\n",
    "    #Update the metrics tracking the loss and accuracy\n",
    "    val_accuracy_results.update_state(y, val_logits)\n",
    "    val_loss_results.update_state(val_loss_value)\n",
    "    \n",
    "\n",
    "\n",
    "#Training loop\n",
    "\n",
    "import time\n",
    "\n",
    "#Preparing the lists for storing the categorical cross entropy loss and accuracy\n",
    "\n",
    "#Number of epochs\n",
    "train_loss_results = keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy_results = keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "val_loss_results = keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy_results = keras.metrics.CategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "#List of the training loss for each epoch\n",
    "train_loss_list = []\n",
    "#List of the training accuracy for each epoch\n",
    "train_accuracy_list = []\n",
    "#List of the validation loss for each epoch\n",
    "val_loss_list = []\n",
    "#List of the validation accuracy for each epoch\n",
    "val_accuracy_list = []\n",
    "\n",
    "\n",
    "\n",
    "#Number of epochs\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    #Iterates over the batches of the dataset\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        #Logging every 200 batches\n",
    "        if step % 200 == 0:\n",
    "            print(\"Training loss (for one batch) at step %d: %.4f\" % (step, float(loss_value)))\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    #Display metrics at the end of each epoch\n",
    "    train_loss = train_loss_results.result()\n",
    "    train_accuracy = train_accuracy_results.result()\n",
    "    print(\"Training loss at epoch %d: %.4f\" % (epoch, float(train_loss)))\n",
    "    print(\"Training accuracy at epoch %d: %.4f\" % (epoch, float(train_accuracy)))\n",
    "\n",
    "\n",
    "    #Add the training loss and accuracy to the lists\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "\n",
    "    #Reset the metrics for the next epoch\n",
    "    train_loss_results.reset_states()\n",
    "    train_accuracy_results.reset_states()\n",
    "\n",
    "    #Validation loop\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "        \n",
    "\n",
    "    #Display validation metrics at the end of each epoch\n",
    "    val_loss = val_loss_results.result()\n",
    "    val_accuracy = val_accuracy_results.result()\n",
    "    print(\"Validation loss at epoch %d: %.4f\" % (epoch, float(val_loss)))\n",
    "    print(\"Validation accuracy at epoch %d: %.4f\" % (epoch, float(val_accuracy)))\n",
    "\n",
    "    #Add the validation loss and accuracy to the lists\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "    #Reset the metrics for the next epoch\n",
    "    val_loss_results.reset_states()\n",
    "    val_accuracy_results.reset_states()\n",
    "\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Plotting the results\n",
    "############################################\n",
    "\n",
    "#Plotting the training loss and validation loss\n",
    "plt.plot(train_loss_list, label='Training loss')\n",
    "plt.plot(val_loss_list, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and validation loss over epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Plotting the training accuracy and validation accuracy\n",
    "plt.plot(train_accuracy_list, label='Training accuracy')\n",
    "plt.plot(val_accuracy_list, label='Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and validation accuracy over epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SAVING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Saving the model\n",
    "############################################\n",
    "\n",
    "file_name = ['q1_model_cifar10.h5', 'q1_model_mnist.h5']\n",
    "\n",
    "#CHOOSE THE MODEL TO SAVE BY CHANGING THE INDEX OF THE FILE_NAME LIST\n",
    "model.save(file_name[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LOADING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Loading the model\n",
    "############################################\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#CHOOSE THE MODEL TO LOAD BY CHANGING THE INDEX OF THE FILE_NAME LIST\n",
    "model = load_model(file_name[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>EVALUATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Testing the model\n",
    "############################################\n",
    "\n",
    "#Loading the test dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "#Computing the accuracy on the test set\n",
    "test_accuracy = keras.metrics.CategoricalAccuracy()\n",
    "for (x_batch_test, y_batch_test) in test_dataset:\n",
    "    test_logits = model(x_batch_test, training=False)\n",
    "    test_accuracy.update_state(y_batch_test, test_logits)\n",
    "print(\"Test set accuracy: %.4f\" % test_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b><font color='orange'>QUESTION 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions for question 2:\n",
    "#Implement and test same network but without using any deep learning framework.\n",
    "#The computation graph and its forward and backward traversals should be hardcoded without having to support dynamic configurations.\n",
    "#The network should be able to handle any number of layers and any number of neurons per layer.\n",
    "#The implementation should include the following:\n",
    "# - a loss and evaluation function\n",
    "# - a class for each node in the graph with methods for computing the forward and backward pass\n",
    "# - forward and backward traversal of the computation graph where data batches are pushed forward and gradient loss values are pushed backward\n",
    "# - implementation of a stochastic gradient descent optimizer with learning rate and decay parameters for updating the weights of the network\n",
    "\n",
    "\n",
    "############################################\n",
    "# Importing the libraries\n",
    "############################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CLASS IMPLEMENTATON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Defining the classes for the nodes of the graph\n",
    "############################################\n",
    "\n",
    "#Class for the Input node\n",
    "class Input:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.input_grad = None\n",
    "        self.output_grad = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = np.array(input)\n",
    "        self.output = input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        self.output_grad = output_grad\n",
    "        self.input_grad = output_grad\n",
    "        return self.input_grad\n",
    "\n",
    "\n",
    "#Class for the Addition node\n",
    "class Addition:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.input_grad = None\n",
    "        self.output_grad = None\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        self.input1 = np.array(input1)\n",
    "        self.input2 = np.array(input2)\n",
    "        self.output = self.input1 + self.input2\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, output_grad):\n",
    "        self.output_grad = output_grad\n",
    "        self.input_grad1 = output_grad\n",
    "        self.input_grad2 = output_grad\n",
    "        return self.input_grad1, self.input_grad2\n",
    "\n",
    "    \n",
    "\n",
    "#Class for the Multiplication node (all the inputs and outputs are tensors)\n",
    "class Multiplication:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.input_grad = None\n",
    "        self.output_grad = None\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        self.input1 = np.array(input1)\n",
    "        self.input2 = np.array(input2)\n",
    "        self.output = np.dot(self.input1, self.input2)\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, output_grad):\n",
    "        self.output_grad = output_grad\n",
    "        self.input_grad1 = np.dot(output_grad, self.input2.T)\n",
    "        self.input_grad2 = np.dot(self.input1.T, output_grad)\n",
    "        return self.input_grad1, self.input_grad2\n",
    "\n",
    "\n",
    "\n",
    "#Class for the Sigmoid node (activation function) (all the inputs and outputs are tensors)\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.input_grad = None\n",
    "        self.output_grad = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = np.array(input)\n",
    "        self.output = 1/(1 + np.exp(-self.input))\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, output_grad):\n",
    "        self.output_grad = output_grad\n",
    "        self.input_grad = self.output_grad * self.output * (1 - self.output)\n",
    "        return self.input_grad\n",
    "    \n",
    "\n",
    "#Class for the Softmax node (activation function) (all the inputs and outputs are tensors)\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.input_grad = None\n",
    "        self.output_grad = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = np.array(input)\n",
    "        self.output = np.exp(self.input)/np.sum(np.exp(self.input), axis=1, keepdims=True)\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, output_grad):\n",
    "        self.output_grad = output_grad\n",
    "        self.input_grad = self.output_grad * self.output * (1 - self.output)\n",
    "        return self.input_grad\n",
    "\n",
    "\n",
    "#Class for the Cross Entropy node (loss function) (all the inputs and outputs are tensors)\n",
    "class CrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.input_grad = None\n",
    "        self.output_grad = None\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        self.input1 = np.array(input1)\n",
    "        self.input2 = np.array(input2)\n",
    "        self.output = -np.sum(self.input2 * np.log(self.input1), axis=1, keepdims=True)\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, output_grad):\n",
    "        self.output_grad = output_grad\n",
    "        self.input_grad1 = -self.input2 + (self.input1)\n",
    "        self.input_grad2 = self.input1\n",
    "        return self.input_grad1, self.input_grad2\n",
    "\n",
    "    def accuracy(self):\n",
    "        return np.sum(np.argmax(self.input1, axis=1) == np.argmax(self.output, axis=1))/len(self.input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>GRAPH CLASS IMPLEMENTATON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Definin the computation graph class\n",
    "############################################\n",
    "\n",
    "#Class for the computation graph\n",
    "class Graph:\n",
    "\n",
    "    # CONSTRUCTOR\n",
    "    def __init__(self, X, target, val, val_target, number_of_neurons):\n",
    "        self.X = X\n",
    "        self.target_values = target\n",
    "        self.val = val\n",
    "        self.val_target_values = val_target\n",
    "        #Defining the nodes of the graph:\n",
    "        #Input layer\n",
    "        self.input = Input()\n",
    "        #Weights\n",
    "        self.w1 = Input()\n",
    "        self.w2 = Input()\n",
    "        self.w3 = Input()\n",
    "        #Bias\n",
    "        self.b1 = Input()\n",
    "        self.b2 = Input()\n",
    "        self.b3 = Input()\n",
    "        #Hidden layer 1\n",
    "        self.mul1 = Multiplication()\n",
    "        self.add1 = Addition()\n",
    "        self.sig1 = Sigmoid()\n",
    "        #Hidden layer 2\n",
    "        self.mul2 = Multiplication()\n",
    "        self.add2 = Addition()\n",
    "        self.sig2 = Sigmoid()\n",
    "        #Output layer\n",
    "        self.mul3 = Multiplication()\n",
    "        self.add3 = Addition()\n",
    "        self.softmax = Softmax()\n",
    "        #Target layer\n",
    "        self.target = Input()\n",
    "        #Loss function\n",
    "        self.cross_entropy = CrossEntropy()\n",
    "\n",
    "        #Initializing the weights and biases using input shape and 2 hidden layers with 10 neurons each\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.w1_input = np.random.randn(self.X.shape[1], self.number_of_neurons)\n",
    "        self.w2_input = np.random.randn(self.number_of_neurons, self.number_of_neurons)\n",
    "        self.w3_input = np.random.randn(self.number_of_neurons, self.target_values.shape[1])\n",
    "        self.b1_input = np.random.randn(1, self.number_of_neurons)\n",
    "        self.b2_input = np.random.randn(1, self.number_of_neurons)\n",
    "        self.b3_input = np.random.randn(1, self.target_values.shape[1])\n",
    "\n",
    "        #Initializing lists for the metrics\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "    # FORWARD PASS\n",
    "    def forward(self, input_arg = None, target_arg = None, predict=False):\n",
    "        #Forward pass\n",
    "        if input_arg is not None:\n",
    "            self.input1 = self.input.forward(input_arg)\n",
    "        else:\n",
    "            self.input1 = self.input.forward(self.X)\n",
    "        self.w1_input = self.w1.forward(self.w1_input)\n",
    "        self.w2_input = self.w2.forward(self.w2_input)\n",
    "        self.w3_input = self.w3.forward(self.w3_input)\n",
    "        self.b1_input = self.b1.forward(self.b1_input)\n",
    "        self.b2_input = self.b2.forward(self.b2_input)\n",
    "        self.b3_input = self.b3.forward(self.b3_input)\n",
    "        self.mul1_output = self.mul1.forward(self.input1, self.w1_input)\n",
    "        self.add1_output = self.add1.forward(self.mul1_output, self.b1_input)\n",
    "        self.sig1_output = self.sig1.forward(self.add1_output)\n",
    "        self.mul2_output = self.mul2.forward(self.sig1_output, self.w2_input)\n",
    "        self.add2_output = self.add2.forward(self.mul2_output, self.b2_input)\n",
    "        self.sig2_output = self.sig2.forward(self.add2_output)\n",
    "        self.mul3_output = self.mul3.forward(self.sig2_output, self.w3_input)\n",
    "        self.add3_output = self.add3.forward(self.mul3_output, self.b3_input)\n",
    "        self.softmax_output = self.softmax.forward(self.add3_output)\n",
    "        if target_arg is not None:\n",
    "            self.target_input = self.target.forward(target_arg)\n",
    "        else:\n",
    "            self.target_input = self.target.forward(self.target_values)\n",
    "        if predict == False:\n",
    "            self.cross_entropy_output = self.cross_entropy.forward(self.softmax_output, self.target_input)\n",
    "            return self.cross_entropy_output\n",
    "        else:\n",
    "            return self.softmax_output\n",
    "\n",
    "\n",
    "\n",
    "    # BACKWARD PASS\n",
    "    def backward(self):\n",
    "        #Backward pass\n",
    "        self.cross_entropy_input_grad1, self.cross_entropy_input_grad2 = self.cross_entropy.backward(1)\n",
    "        self.target_input_grad = self.target.backward(self.cross_entropy_input_grad2)\n",
    "        self.softmax_input_grad = self.softmax.backward(self.cross_entropy_input_grad1)\n",
    "        self.add3_input_grad_1, self.add3_input_grad_2 = self.add3.backward(self.softmax_input_grad)\n",
    "        self.mul3_input_grad_1, self.mul3_input_grad_2 = self.mul3.backward(self.add3_input_grad_1)\n",
    "        self.sig2_input_grad = self.sig2.backward(self.mul3_input_grad_1)\n",
    "        self.add2_input_grad_1, self.add2_input_grad_2 = self.add2.backward(self.sig2_input_grad)\n",
    "        self.mul2_input_grad_1, self.mul2_input_grad_2 = self.mul2.backward(self.add2_input_grad_1)\n",
    "        self.sig1_input_grad = self.sig1.backward(self.mul2_input_grad_1)\n",
    "        self.add1_input_grad_1, self.add1_input_grad_2 = self.add1.backward(self.sig1_input_grad)\n",
    "        self.mul1_input_grad_1, self.mul1_input_grad_2 = self.mul1.backward(self.add1_input_grad_1)\n",
    "        self.input_input_grad = self.input.backward(self.mul1_input_grad_1)\n",
    "        self.w1_input_grad = self.w1.backward(self.mul1_input_grad_2)\n",
    "        self.w2_input_grad = self.w2.backward(self.mul2_input_grad_2)\n",
    "        self.w3_input_grad = self.w3.backward(self.mul3_input_grad_2)\n",
    "        self.b1_input_grad = self.b1.backward(self.add1_input_grad_2)\n",
    "        self.b2_input_grad = self.b2.backward(self.add2_input_grad_2)\n",
    "        self.b3_input_grad = self.b3.backward(self.add3_input_grad_2)\n",
    "        #bias gradients should be summed up\n",
    "        self.b1_input_grad = np.sum(self.b1_input_grad, axis=0)\n",
    "        self.b2_input_grad = np.sum(self.b2_input_grad, axis=0)\n",
    "        self.b3_input_grad = np.sum(self.b3_input_grad, axis=0)\n",
    "        return self.w1_input_grad, self.w2_input_grad, self.w3_input_grad, self.b1_input_grad, self.b2_input_grad, self.b3_input_grad\n",
    "\n",
    "\n",
    "    # UPDATE WEIGHTS\n",
    "    def update(self, w1_input_grad, w2_input_grad, w3_input_grad, b1_input_grad, b2_input_grad, b3_input_grad, lr, decay, iteration):\n",
    "        #Updating the weights and biases\n",
    "        self.w1_input -= self.lr * 1/(1 + decay * iteration) * w1_input_grad\n",
    "        self.w2_input -= self.lr * 1/(1 + decay * iteration) * w2_input_grad\n",
    "        self.w3_input -= self.lr * 1/(1 + decay * iteration) * w3_input_grad\n",
    "        self.b1_input -= self.lr * 1/(1 + decay * iteration) * b1_input_grad\n",
    "        self.b2_input -= self.lr * 1/(1 + decay * iteration) * b2_input_grad\n",
    "        self.b3_input -= self.lr * 1/(1 + decay * iteration) * b3_input_grad\n",
    "\n",
    "\n",
    "\n",
    "    # TRAINING\n",
    "    def train(self, epochs, batch_size, lr = 0.01, decay = 0.0001):\n",
    "        #Training the model\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        \n",
    "        #Batching input data and target values into batches of size batch_size whith shuffling as an argument to the batch functio\n",
    "        def batch_data(input_arg, target_arg, batch_size, shuffle = True):\n",
    "            if shuffle == True:\n",
    "                indices = np.random.permutation(len(input_arg))\n",
    "            else:\n",
    "                indices = np.arange(len(input_arg))\n",
    "            input_batch = []\n",
    "            target_batch = []\n",
    "            #handling the case when the batch size is not a divisor of the number of samples and the last batch is smaller than the rest\n",
    "            if len(input_arg) % batch_size != 0:\n",
    "                for i in range(len(input_arg) // batch_size + 1):\n",
    "                    input_batch.append(input_arg[indices[i * batch_size : min((i + 1) * batch_size, len(input_arg))]])\n",
    "                    target_batch.append(target_arg[indices[i * batch_size : min((i + 1) * batch_size, len(input_arg))]])\n",
    "            else:\n",
    "                for i in range(len(input) // batch_size):\n",
    "                    input_batch.append(input_arg[indices[i * batch_size : (i + 1) * batch_size]])\n",
    "                    target_batch.append(target_arg[indices[i * batch_size : (i + 1) * batch_size]])\n",
    "            return input_batch, target_batch\n",
    "\n",
    "\n",
    "        #Batching the input data and target values\n",
    "        training_input_batches, training_target_batches = batch_data(self.X, self.target_values, self.batch_size)\n",
    "        validation_input_batches, validation_target_batches = batch_data(self.val, self.val_target_values, self.batch_size, shuffle = False)\n",
    "\n",
    "        #Looping through the epochs\n",
    "        for i in range(self.epochs):\n",
    "            #starting time\n",
    "            start = time.time()\n",
    "\n",
    "            #List of losses for each batch\n",
    "            training_losses_batches = []\n",
    "            validation_losses_batches = []\n",
    "            #List of accuracies for each batch\n",
    "            training_accuracies_batches = []\n",
    "            validation_accuracies_batches = []\n",
    "\n",
    "            #Looping through the batches\n",
    "            for training_input_batch, training_target_batch in zip(training_input_batches, training_target_batches):\n",
    "                #Forward pass\n",
    "                self.forward(training_input_batch, training_target_batch)\n",
    "                #Backward pass\n",
    "                self.backward()\n",
    "                #Updating the weights and biases\n",
    "                self.update(self.w1_input_grad, self.w2_input_grad, self.w3_input_grad, self.b1_input_grad, self.b2_input_grad, self.b3_input_grad, self.lr, self.decay, i)\n",
    "\n",
    "                #Appending the losses and accuracies to the lists\n",
    "                loss_value = np.sum(self.cross_entropy_output)\n",
    "                training_losses_batches.append(loss_value)\n",
    "\n",
    "                accuracy_value = self.cross_entropy.accuracy()\n",
    "                training_accuracies_batches.append(accuracy_value)\n",
    "\n",
    "            #Looping through the batches\n",
    "            for validation_input_batch, validation_target_batch in zip(validation_input_batches, validation_target_batches):\n",
    "                #Forward pass\n",
    "                self.forward(validation_input_batch, validation_target_batch)\n",
    "\n",
    "                #Appending the losses and accuracies to the lists\n",
    "                loss_value = np.sum(self.cross_entropy_output)\n",
    "                validation_losses_batches.append(loss_value)\n",
    "\n",
    "                accuracy_value = self.cross_entropy.accuracy()\n",
    "                validation_accuracies_batches.append(accuracy_value)\n",
    "\n",
    "            #Calculating the mean of the losses and accuracies\n",
    "            training_loss = np.mean(training_losses_batches)\n",
    "            validation_loss = np.mean(validation_losses_batches)\n",
    "            training_accuracy = np.mean(training_accuracies_batches)\n",
    "            validation_accuracy = np.mean(validation_accuracies_batches)\n",
    "\n",
    "            #Appending the losses and accuracies to the lists\n",
    "            self.losses.append(training_loss)\n",
    "            self.val_losses.append(validation_loss)\n",
    "            self.accuracies.append(training_accuracy)\n",
    "            self.val_accuracies.append(validation_accuracy)\n",
    "\n",
    "            #Printing the losses and accuracies\n",
    "            print(\"Epoch: \", i+1, \"/\", self.epochs ,\"\\n\\n\",\n",
    "                    \"Training loss: \", training_loss, \" | \",\n",
    "                    \"Validation loss: \", validation_loss, \" | \",\n",
    "                    \"Training accuracy: \", training_accuracy, \" | \",\n",
    "                    \"Validation accuracy: \", validation_accuracy,)\n",
    "            #Ending time\n",
    "            end = time.time()\n",
    "            print(\"\\nTime: \", end - start, \"s\")\n",
    "\n",
    "            print(\"\\n------------------------------------------------------------\\n\")       \n",
    "\n",
    "\n",
    "        return self.losses, self.accuracies, self.val_losses, self.val_accuracies\n",
    "\n",
    "    # PREDICTION\n",
    "    def predict(self, X):\n",
    "        #Predicting the output\n",
    "        self.forward(input_arg = X, predict = True)\n",
    "        return self.softmax_output\n",
    "\n",
    "    # EVALUATION\n",
    "    def evaluate(self, X, target_values):\n",
    "        #Evaluating the model\n",
    "        self.forward(input_arg = X, target_arg = target_values)\n",
    "        return self.cross_entropy.accuracy()\n",
    "\n",
    "\n",
    "    # SAVING THE MODEL\n",
    "    def save(self, filename):\n",
    "        #Saving the model\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "\n",
    "    #LOADING THE MODEL\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        #Loading the model\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CIFAR10 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# CIFAR10 - Preprocessing of the data\n",
    "############################################\n",
    "\n",
    "#Loading the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#Selection of a subset of 3 classes of the dataset\n",
    "def select_subset(x_train, y_train, classes):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    for i in range((len(x_train))):\n",
    "        for j in range(len(classes)):\n",
    "            if y_train[i] == classes[j]:\n",
    "                x_train_temp.append(x_train[i])\n",
    "                y_train_temp.append(y_train[i])\n",
    "    return np.array(x_train_temp), np.array(y_train_temp)\n",
    "\n",
    "#Selection of the classes\n",
    "classes = [0, 1, 2]\n",
    "x_train, y_train = select_subset(x_train, y_train, classes)\n",
    "x_test, y_test = select_subset(x_test, y_test, classes)\n",
    "\n",
    "\n",
    "#Order the training data by class by returning an array of n classes\n",
    "def order_data(x_train, y_train):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    for i in range(len(classes)):\n",
    "        x_train_temp.append([])\n",
    "        y_train_temp.append([])\n",
    "    for i in range(len(x_train)):\n",
    "        for j in range(len(classes)):\n",
    "            if y_train[i] == classes[j]:\n",
    "                x_train_temp[j].append(x_train[i])\n",
    "                y_train_temp[j].append(y_train[i])\n",
    "    return x_train_temp, y_train_temp\n",
    "\n",
    "x_train_ordered, y_train_ordered = order_data(x_train, y_train)\n",
    "\n",
    "#Splitting the dataset into the Training set and Validation set\n",
    "#Training set and Validation set should have the same distribution of classes\n",
    "#Split ratio: 80% training set, 20% validation set\n",
    "def split_data(x_train_ordered, y_train_ordered, split_ratio):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    x_val_temp = []\n",
    "    y_val_temp = []\n",
    "    for i in range(len(x_train_ordered)):\n",
    "        for j in range(len(x_train_ordered[i])):\n",
    "            if j < len(x_train_ordered[i])*split_ratio:\n",
    "                x_train_temp.append(x_train_ordered[i][j])\n",
    "                y_train_temp.append(y_train_ordered[i][j])\n",
    "            else:\n",
    "                x_val_temp.append(x_train_ordered[i][j])\n",
    "                y_val_temp.append(y_train_ordered[i][j])\n",
    "    return np.array(x_train_temp), np.array(y_train_temp), np.array(x_val_temp), np.array(y_val_temp)\n",
    "\n",
    "\n",
    "x_train, y_train, x_val, y_val = split_data(x_train_ordered, y_train_ordered, 0.8)\n",
    "\n",
    "#Shuffling the training set and the validation set\n",
    "def shuffle_data(x_train, y_train, x_val, y_val):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    x_val_temp = []\n",
    "    y_val_temp = []\n",
    "    for i in range(len(x_train)):\n",
    "        x_train_temp.append(x_train[i])\n",
    "        y_train_temp.append(y_train[i])\n",
    "    for i in range(len(x_val)):\n",
    "        x_val_temp.append(x_val[i])\n",
    "        y_val_temp.append(y_val[i])\n",
    "    np.random.shuffle(x_train_temp)\n",
    "    np.random.shuffle(y_train_temp)\n",
    "    np.random.shuffle(x_val_temp)\n",
    "    np.random.shuffle(y_val_temp)\n",
    "    return np.array(x_train_temp), np.array(y_train_temp), np.array(x_val_temp), np.array(y_val_temp)\n",
    "\n",
    "x_train, y_train, x_val, y_val = shuffle_data(x_train, y_train, x_val, y_val)\n",
    "\n",
    "#Reshaping the images\n",
    "x_train = x_train.reshape(x_train.shape[0], 32*32*3)\n",
    "x_val = x_val.reshape(x_val.shape[0], 32*32*3)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32*32*3)\n",
    "\n",
    "#Encoding the labels using categorical encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=3)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MNIST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# MNIST - Preprocessing of the data\n",
    "############################################\n",
    "\n",
    "#Loading the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "#Selection of a subset of 3 classes of the dataset\n",
    "def select_subset(x_train, y_train, classes):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    for i in range((len(x_train))):\n",
    "        for j in range(len(classes)):\n",
    "            if y_train[i] == classes[j]:\n",
    "                x_train_temp.append(x_train[i])\n",
    "                y_train_temp.append(y_train[i])\n",
    "    return np.array(x_train_temp), np.array(y_train_temp)\n",
    "\n",
    "#Selection of the classes\n",
    "classes = [0, 1, 2]\n",
    "x_train, y_train = select_subset(x_train, y_train, classes)\n",
    "x_test, y_test = select_subset(x_test, y_test, classes)\n",
    "\n",
    "#Order the training data by class by returning an array of n classes\n",
    "def order_data(x_train, y_train):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    for i in range(len(classes)):\n",
    "        x_train_temp.append([])\n",
    "        y_train_temp.append([])\n",
    "    for i in range(len(x_train)):\n",
    "        for j in range(len(classes)):\n",
    "            if y_train[i] == classes[j]:\n",
    "                x_train_temp[j].append(x_train[i])\n",
    "                y_train_temp[j].append(y_train[i])\n",
    "    return x_train_temp, y_train_temp\n",
    "\n",
    "x_train_ordered, y_train_ordered = order_data(x_train, y_train)\n",
    "\n",
    "#Splitting the dataset into the Training set and Validation set\n",
    "#Training set and Validation set should have the same distribution of classes\n",
    "#Split ratio: 80% training set, 20% validation set\n",
    "def split_data(x_train_ordered, y_train_ordered, split_ratio):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    x_val_temp = []\n",
    "    y_val_temp = []\n",
    "    for i in range(len(x_train_ordered)):\n",
    "        for j in range(len(x_train_ordered[i])):\n",
    "            if j < len(x_train_ordered[i])*split_ratio:\n",
    "                x_train_temp.append(x_train_ordered[i][j])\n",
    "                y_train_temp.append(y_train_ordered[i][j])\n",
    "            else:\n",
    "                x_val_temp.append(x_train_ordered[i][j])\n",
    "                y_val_temp.append(y_train_ordered[i][j])\n",
    "    return np.array(x_train_temp), np.array(y_train_temp), np.array(x_val_temp), np.array(y_val_temp)\n",
    "\n",
    "\n",
    "x_train, y_train, x_val, y_val = split_data(x_train_ordered, y_train_ordered, 0.8)\n",
    "\n",
    "#Shuffling the training set and the validation set\n",
    "def shuffle_data(x_train, y_train, x_val, y_val):\n",
    "    x_train_temp = []\n",
    "    y_train_temp = []\n",
    "    x_val_temp = []\n",
    "    y_val_temp = []\n",
    "    for i in range(len(x_train)):\n",
    "        x_train_temp.append(x_train[i])\n",
    "        y_train_temp.append(y_train[i])\n",
    "    for i in range(len(x_val)):\n",
    "        x_val_temp.append(x_val[i])\n",
    "        y_val_temp.append(y_val[i])\n",
    "    np.random.shuffle(x_train_temp)\n",
    "    np.random.shuffle(y_train_temp)\n",
    "    np.random.shuffle(x_val_temp)\n",
    "    np.random.shuffle(y_val_temp)\n",
    "    return np.array(x_train_temp), np.array(y_train_temp), np.array(x_val_temp), np.array(y_val_temp)\n",
    "\n",
    "x_train, y_train, x_val, y_val = shuffle_data(x_train, y_train, x_val, y_val)\n",
    "\n",
    "#Reshaping the images\n",
    "x_train = x_train.reshape(x_train.shape[0], 28*28)\n",
    "x_val = x_val.reshape(x_val.shape[0], 28*28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28*28)\n",
    "\n",
    "#Encoding the labels using categorical encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=3)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>EXECUTE THE NEXT CELL ONLY IF YOU WANT TO TRAIN ON THE ENTIRE TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Training the model using the entire dataset\n",
    "############################################\n",
    "\n",
    "#Training the model using the entire dataset by merging the training and validation sets\n",
    "x_train = np.concatenate((x_train, x_val), axis=0)\n",
    "y_train = np.concatenate((y_train, y_val), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Training the model\n",
    "############################################\n",
    "\n",
    "model = Graph(x_train, y_train, x_val, y_val, number_of_neurons=100)\n",
    "losses, accuracies, val_losses, val_accuracies = model.train(epochs = 15, batch_size = 64, lr = 0.001, decay = 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Plotting the loss and accuracy of the training set and validation set\n",
    "############################################\n",
    "\n",
    "plt.plot(losses, label = \"Training Loss\")\n",
    "plt.plot(val_losses, label = \"Validation Loss\")\n",
    "plt.title(\"Loss of the training set and validation set\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(accuracies, label = \"Training accuracy\")\n",
    "plt.plot(val_accuracies, label = \"Validation accuracy\")\n",
    "plt.title(\"Accuracy of the training set and validation set\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SAVING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Saving the model\n",
    "############################################\n",
    "\n",
    "file_name = ['q2_model_cifar10.pkl', 'q2_model_mnist.pkl']\n",
    "\n",
    "#CHOOSE THE MODEL TO SAVE BY CHANGING THE INDEX OF THE FILE_NAME LIST\n",
    "model.save(file_name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LOADING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Loading the model\n",
    "############################################\n",
    "\n",
    "#CHOOSE THE MODEL TO LOAD BY CHANGING THE INDEX OF THE FILE_NAME LIST\n",
    "model = Graph.load(file_name[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>EVALUATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Evaluation of the model\n",
    "############################################\n",
    "\n",
    "#Evaluation of the model on the test set\n",
    "accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Accuracy on the test set: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
